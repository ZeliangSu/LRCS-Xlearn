# import everything
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import os
import multiprocessing as mp
from tqdm import tqdm
import h5py
from PIL import Image



def parse_h5(args):
    '''
    parser that return the input images and  output labels

    input:
    -------
    args: (bytes literal) nested args e.g. b'./proc/train/96/0.h5, 96'

    output:
    -------
    X: (numpy ndarray) normalized and reshaped array as dataformat 'NHWC'
    y: (numpy ndarray) normalized and reshaped array as dataformat 'NHWC'
    '''

    patch_size = 100
    with h5py.File(args.decode('utf-8'), 'r') as f:
        X = f['X'][:].reshape(patch_size, patch_size, 1)
        y = f['y'][:].reshape(patch_size, patch_size, 1)
        return _minmaxscalar(X), y  #can't do minmaxscalar for y


def _minmaxscalar(ndarray, dtype=np.float32):
    '''
    func normalize values of a ndarray into value within interval of 0 to 1

    input:
    -------
    ndarray: (numpy ndarray) input array to be normalized
    dtype: (dtype of numpy) data type of the output of this function

    output:
    -------
    scaled: (numpy ndarray) output normalized array
    '''
    scaled = np.array((ndarray - np.min(ndarray)) / (np.max(ndarray) - np.min(ndarray)), dtype=dtype)
    return scaled


def _pyfn_wrapper(args):
    '''
    input:
    -------
    filename: (tf.data.Dataset)  Tensors of strings

    output:
    -------
    function: (function) tensorflow's pythonic function with its arguements
    '''

    return tf.py_func(parse_h5,  #wrapped pythonic function
                      [args],
                      [tf.float32, tf.float32]  #[input, output] dtype
                      )

totrain_files_ph = tf.placeholder(tf.string, [None], name='totrain_files')

# transform on tf.data.Dataset:
dataset = tf.data.Dataset.from_tensor_slices(totrain_files_ph)

# prepare input pipeline
# apply list of file names to the py function wrapper for reading files
dataset = dataset.map(_pyfn_wrapper, num_parallel_calls=mp.cpu_count()) #mp.cpu_count() = 48

# construct batch size
dataset = dataset.batch(1).prefetch(mp.cpu_count())

# initialize iterator
iterator = dataset.make_initializable_iterator()
iterator_initialize_op = iterator.initializer

# input
# get image and labels
image_getnext_op, label_getnext_op = iterator.get_next()

# first instance get the image and second get the label associated
# inject into model
with tf.name_scope("Conv1"):
    # initialize weights
    W = tf.get_variable("W", shape=[3, 3, 1, 1],
                        initializer=tf.contrib.layers.xavier_initializer())

    # initialize bias
    b = tf.get_variable("b", shape=[1], initializer=tf.contrib.layers.xavier_initializer())

    # draw convolutional layer
    layer1 = tf.nn.conv2d(image_getnext_op, W, strides=[1, 1, 1, 1], padding='SAME') + b

    # activation function
    logits = tf.nn.relu(layer1, name='logits')

#
loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=label_getnext_op, predictions=logits))
train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)


# create list of file names: ['test_0.h5', 'test_1.h5', ...]
totrain_files = [os.path.join('./dummy/', f) for f in os.listdir('./dummy/') if f.endswith('.h5')] #list: ['test_0.h5', 'test_1.h5', ...]
epoch_length = len(totrain_files) # each epoch ==> 100 steps (100 h5 files)

print(totrain_files)

# init session

with tf.Session() as sess:
    # init global variable and iterator
    # run train operation
    sess.run([iterator_initialize_op, tf.global_variables_initializer()], feed_dict={totrain_files_ph: totrain_files})

    for step in range(epoch_length):
        sess.run(train_op)

        path = os.path.join(os.getcwd(), 'simple')
        inputs_dict = {'totrain_files_ph': totrain_files_ph}
        outputs_dict = {'logits': logits}
        tf.saved_model.simple_save(sess, path, inputs_dict, outputs_dict)

        # see all names of nodes
        #[n.name for n in tf.get_default_graph().as_graph_def().node]

        # most important ones : 'IteratorGetNext_1' , 'Conv/prediction' , 'Operations/difference'

        sess.close()

        # checkpoint variable inspection
        #from tensorflow.python.tools import inspect_checkpoint as ckpt

        #ckpt.print_tensors_in_checkpoint_file("./dummy/99.ckpt", tensor_name='', all_tensors=True)  # you can also run for other step by changing to `step.ckpt`

        if not os.path.exists('./pred_results'):
            os.makedirs('./pred_results')
        graph = tf.Graph()
        sess = tf.Session(graph=graph2)

        # restore last step's weights
        # with tf.Session() as sess:
        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], export_dir=path)

        # create list of file names: ['test_0.h5', 'test_1.h5', ...]
        totest_files = [os.path.join('./predict/', f) for f in os.listdir('./predict/') if f.endswith('.h5')]
        epoch_length = len(totest_files)  # each epoch ==> 100 steps (100 h5 files)
        print(totrain_files)

        # transform on tf.data.Dataset:
        dataset = tf.data.Dataset.from_tensor_slices(totest_files)

    # create predict operation
    # Get restored placeholders
    totest_files_ph = graph.get_tensor_by_name('Inputpipeline/totrain_files:0')

    # W = graph2.get_tensor_by_name('W:0')
    # Get restored model output
    restored_logits = graph.get_tensor_by_name('Conv1/logits:0')

    ####################################
    diff_tensor = graph.get_tensor_by_name('Operations/difference:0')

    # Get dataset initializing operation
    iterator_initialize_op = graph.get_operation_by_name('Inputpipeline/predict_iter_init_op')

    # sess.run(predict operation)
    # with tf.Session() as sess:
    sess.run(iterator_initialize_op , feed_dict={totest_files_ph:totest_files} )

    #faire un aller
    for step in range(epoch_length):
        logits, difference = sess.run([restored_logits, diff_tensor])
        print(logits.shape)

        with h5py.File('./pred_results/{}.h5'.format(step), 'w') as f:
            a = f.create_dataset('logits', (100, 100), dtype='float32')
            a[:] = logits.reshape(100, 100)
            b = f.create_dataset('difference', (100, 100), dtype='float32', data=difference)
            b[:] = difference.reshape(100, 100)

        hdf = h5py.File("49.h5", 'r')
        array = hdf["./pred_results/{}.h5"][:]
        img = Image.fromarray(array.astype('uint8'), 'RGB')
        img.save("yourimage.thumbnail", "JPEG")
        img.show()
