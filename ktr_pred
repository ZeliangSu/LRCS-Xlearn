# import everything
import tensorflow as tf
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import os
import multiprocessing as mp
from tqdm import tqdm
import h5py
%matplotlib inline
from tensorflow.saved_model import tag_constants

def parse_h5(args):
    '''
    parser that return the input images and  output labels

    input:
    -------
    args: (bytes literal) nested args e.g. b'./proc/train/96/0.h5, 96'

    output:
    -------
    X: (numpy ndarray) normalized and reshaped array as dataformat 'NHWC'
    y: (numpy ndarray) normalized and reshaped array as dataformat 'NHWC'
    '''

    patch_size = 100
    with h5py.File(args.decode('utf-8'), 'r') as f:
        X = f['X'][:].reshape(patch_size, patch_size, 1)
        y = f['y'][:].reshape(patch_size, patch_size, 1)
        return _minmaxscalar(X), y  #can't do minmaxscalar for y


def _minmaxscalar(ndarray, dtype=np.float32):
    '''
    func normalize values of a ndarray into value within interval of 0 to 1

    input:
    -------
    ndarray: (numpy ndarray) input array to be normalized
    dtype: (dtype of numpy) data type of the output of this function

    output:
    -------
    scaled: (numpy ndarray) output normalized array
    '''
    scaled = np.array((ndarray - np.min(ndarray)) / (np.max(ndarray) - np.min(ndarray)), dtype=dtype)
    return scaled


def _pyfn_wrapper(args):
    '''
    input:
    -------
    filename: (tf.data.Dataset)  Tensors of strings

    output:
    -------
    function: (function) tensorflow's pythonic function with its arguements
    '''

    return tf.py_func(parse_h5,  #wrapped pythonic function
                      [args],
                      [tf.float32, tf.float32]  #[input, output] dtype
                      )

totrain_files_ph = tf.placeholder(tf.string, [None], name='totrain_files')

# transform on tf.data.Dataset:
dataset = tf.data.Dataset.from_tensor_slices(totrain_files_ph)

# prepare input pipeline
# apply list of file names to the py function wrapper for reading files
dataset = dataset.map(_pyfn_wrapper, num_parallel_calls=mp.cpu_count()) #mp.cpu_count() = 48

# construct batch size
dataset = dataset.batch(1).prefetch(mp.cpu_count())

# initialize iterator
iterator = dataset.make_initializable_iterator()
iterator_initialize_op = iterator.initializer

# input
# get image and labels
image_getnext_op, label_getnext_op = iterator.get_next()
# fisrt instance get the image and second get the label associated

# inject into model
with tf.name_scope("Conv1"):
    # initialize weights
    W = tf.get_variable("W", shape=[3, 3, 1, 1],
                        initializer=tf.contrib.layers.xavier_initializer())

    # initialize bias
    b = tf.get_variable("b", shape=[1], initializer=tf.contrib.layers.xavier_initializer())

    # draw convolutional layer
    layer1 = tf.nn.conv2d(image_getnext_op, W, strides=[1, 1, 1, 1], padding='SAME') + b

    # activation function
    logits = tf.nn.relu(layer1, name='logits')

#
loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=label_getnext_op, predictions=logits))
train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)


# create list of file names: ['test_0.h5', 'test_1.h5', ...]
totrain_files = [os.path.join('./dummy/', f) for f in os.listdir('./dummy/') if f.endswith('.h5')] #list: ['test_0.h5', 'test_1.h5', ...]
epoch_length = len(totrain_files) # each epoch ==> 100 steps (100 h5 files)

print(totrain_files)

# init session

sess = tf.Session()

# init global variable and iterator
# run train operation
sess.run([iterator_initialize_op, tf.global_variables_initializer()], feed_dict={totrain_files_ph: totrain_files})

for step in range(epoch_length):
    sess.run(train_op)

    path = os.path.join(os.getcwd(), 'simple')
    inputs_dict = {'totrain_files_ph': totrain_files_ph}
    outputs_dict = {'logits': logits}
    tf.saved_model.simple_save(sess, path, inputs_dict, outputs_dict)
    # see all names of nodes
    [n.name for n in tf.get_default_graph().as_graph_def().node]
    # most important ones : 'IteratorGetNext_1' , 'Conv/prediction' , 'Operations/difference'
    sess.close()
    # checkpoint variable inspection
    from tensorflow.python.tools import inspect_checkpoint as ckpt

    ckpt.print_tensors_in_checkpoint_file("./dummy/99.ckpt", tensor_name='',
                                          all_tensors=True)  # you can also run for other step by changing to `step.ckpt`
    # restore last step's weights
    with tf.Session() as sess:
        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], export_dir=path)
        # create list of file names: ['test_0.h5', 'test_1.h5', ...]
        totest_files = [os.path.join('./predict/', f) for f in os.listdir('./predict/') if f.endswith('.h5')]
        epoch_length = len(totest_files)  # each epoch ==> 100 steps (100 h5 files)
        print(totrain_files)

        # transform on tf.data.Dataset:
        dataset = tf.data.Dataset.from_tensor_slices(totest_files)

# create predict operation
# Get restored placeholders
totrain_files_ph = graph.get_tensor_by_name('totrain_files_ph')
# Get restored model output
restored_logits = graph.get_tensor_by_name('restored_logits')
#get dataest initialising operation
iterator_initialize_op = graph.get_operation_by_name('iterator_initialize_op')

sess.run( iterator_initialize_op, feed_dict={ totrain_files_ph : totrain_files })

# sess.run(predict operation)
print sess.run(prediction,feed_dict={input:dataset})  #newdata=put your data here

#sess.close()